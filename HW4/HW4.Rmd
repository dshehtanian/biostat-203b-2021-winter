---
title: "Biostat 203B Homework 4"
author: Dominic Shehtanian
subtitle: Due Mar 19 @ 11:59PM
output:
  # ioslides_presentation: default
  html_document:
    toc: true
    toc_depth: 4
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```
                      
Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r setup}
library(tidyverse)
library(lubridate)
library(miceRanger)
library(glmnet)
library(keras)
```

## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 

1. Explain the jargon MCAR, MAR, and MNAR.

**Solution:**

MCAR - Missing completely at random - The probability of being missing is unrelated to the value of the observations.
MAR - Missing at random - The probability of being missing depends only on observed values, not unobserved/missing values. 
MNAR - Missing not at random - The probability of being missing depnends on the value of the missing data.

2. Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

**Solution**:

All missing values in the dataset are replaced by the mean for that variable, then one at a time, each variable with missing observations has those observations set back to missing (from the first imputed values) and the observations are predicted using the other variables in the data set. This is repeated for a set number of cycles.

3. Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

**Solution:**

I filtered extreme values based on doing researching observed ranges for these variables.

References:
Based on [this article](https://pubmed.ncbi.nlm.nih.gov/7741618/#:~:text=The%20highest%20pressure%20recorded%20in,maximal%20lifting%20with%20slow%20exhalation.), it appears that the highest recorded blood pressure was 370.

Based on [this article](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3273956/#:~:text=The%20fastest%20human%20ventricular%20conduction%20rate%20reported%20to%20date%20is,of%20480%20beats%20per%20minute.), it appears the highest recorded heart rate was 480.

Based on [this article](http://www.atcs.jp/pdf/2008_14_3/138.pdf), I chose 60 degrees Fahrenheit as the lowest reasonable temperature reading for a living patient.
```{r}
#Reading in the ICU Cohort Data
icudata <- readRDS("./icu_cohort.rds")
summary(icudata)

#Dropping variables with >5000 NAs
icudata_filtered <- select(icudata, -deathtime, -edregtime, -edouttime, -dod, 
                           -arterial_blood_pressure_systolic, 
                           -arterial_blood_pressure_mean, -lactate, 
                           -days_to_death, -subject_id, -hadm_id, -stay_id,
                           -intime, -outtime, -los, -admittime, -dischtime, 
                           -deathtime, -anchor_age, -anchor_year, 
                           -anchor_year_group, -first_careunit, -last_careunit,
                           -admission_type, -admission_location, 
                           -discharge_location, -hospital_expire_flag)

#Replacing probable data entry errors with NA 
icudata_filtered$non_invasive_blood_pressure_systolic[
  icudata_filtered$non_invasive_blood_pressure_systolic > 370] <- NA

icudata_filtered$non_invasive_blood_pressure_mean[
  icudata_filtered$non_invasive_blood_pressure_mean > 370] <- NA

icudata_filtered$heart_rate[icudata_filtered$heart_rate > 480] <- NA

icudata_filtered$temperature_fahrenheit[
  icudata_filtered$temperature_fahrenheit < 60] <- NA

summary(icudata_filtered)
```

Based on the summary of icudata, I removed deathtime, edregtime, edouttime, dod, arterial blood pressure - systolic, arterial blood pressure - mean, lactate, and days to death for large amounts of missing data. I replaced apparent data entry errors in heart rate, non-invasive blood pressure - systolic, non-invasive blood pressure - mean, and temperature (Farenheit) with `NA`.

4. Impute missing values by `miceRanger` (request $m=3$ datasets). This step is very computational intensive. Make sure to save the imputation results as a file.

```{r}
#max depth needed to be set otherwise my R instance would crash on the 
#teaching server
seqTime <- system.time(
  miceObj <- miceRanger(
    icudata_filtered, m = 3, 
    max.depth = 10, 
    returnModels = TRUE, verbose = TRUE))
miceObj

```

5. Make imputation diagnostic plots and explain what they mean.

```{r}
plotDistributions(miceObj, vars = 'allNumeric')
plotVarConvergence(miceObj, vars = 'allNumeric')
```
**Solution:**
The first set of plots, shows the distribution of the original data (in red) and the imputed data (in black). For most of the variable these match fairly closely. However, for heart rate appears to have very large deviations and respiratory rate and hematocrit have noticeable deviations. This implies that data is not missing completely at random.

The second set of plots shows whether the imputed data has converged. It looks like most of our variables have converged. However, it looks like many variables did not converge, including respiratory rate, temperature (fahrenheit), calcium, and magnesium.

I ran the imputation with a max depth of 20 on my personal computer and many variables still had issues with convergence. I limited the max depth to ten, due to performance issues on the teaching server.

6. Obtain a complete data set by averaging the 3 imputed data sets.
```{r}
icu_imputed <- completeData(miceObj)
#Need to convert to matrix to average
#Reconverted to data frame to use column names in part 2
icu_imputed$Dataset_1 <- dummy_cols(icu_imputed$Dataset_1) %>% select(-insurance, - language, - marital_status, -ethnicity, -gender)
icu_imputed$Dataset_2 <- dummy_cols(icu_imputed$Dataset_2) %>% select(-insurance, - language, - marital_status, -ethnicity, -gender)
icu_imputed$Dataset_3 <- dummy_cols(icu_imputed$Dataset_3) %>% select(-insurance, - language, - marital_status, -ethnicity, -gender)
icu_analysis <- data.frame((data.matrix(icu_imputed$Dataset_1) + 
                              data.matrix(icu_imputed$Dataset_2) + 
                              data.matrix(icu_imputed$Dataset_3))/3)
icu_analysis
```

## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function), (2) logistic regression with lasso penalty (glmnet package), (3) random forest (randomForest package), or (4) neural network.

1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.
```{r}
#Stratifying by 30-day mortality status
d30_true <- icu_analysis[
  icu_analysis$died_within_30d == 1, 1:ncol(icu_analysis)]
d30_false <- icu_analysis[
  icu_analysis$died_within_30d == 0, 1:ncol(icu_analysis)]
#Sampling from to build our training and test sets
train_size30T <- round(nrow(d30_true) * .8)
train_size30F <- round(nrow(d30_false) * .8)
smpl_true <- sample(1:nrow(d30_true), train_size30T)
smpl_false <- sample(1:nrow(d30_false), train_size30F)

#Recombines training data of 30-day mortality TRUE and FALSE
icu_train <- rbind(d30_true[smpl_true,1:ncol(icu_analysis)], 
                   d30_false[smpl_false,1:ncol(icu_analysis)])
#Randomizes the rows, without doing this I got weird validation accuracy
#Without doing this all 30-day mortality TRUE come before FALSE
rows <- sample(nrow(icu_train))
icu_train <- icu_train[rows, ]
#creating x_train and y_train for Neural Network/Logistic Regression
x_train <- model.matrix(died_within_30d ~ -1 + gender_F + gender_M +
                          age_at_adm + marital_status_DIVORCED + 
                          marital_status_MARRIED + marital_status_SINGLE + 
                          marital_status_WIDOWED + 
                          ethnicity_AMERICAN.INDIAN.ALASKA.NATIVE + 
                          ethnicity_ASIAN + ethnicity_BLACK.AFRICAN.AMERICAN + 
                          ethnicity_HISPANIC.LATINO + ethnicity_OTHER + 
                          ethnicity_WHITE + ethnicity_UNABLE.TO.OBTAIN + 
                          ethnicity_UNKNOWN + heart_rate + 
                          non_invasive_blood_pressure_systolic + 
                          non_invasive_blood_pressure_mean +
                          respiratory_rate + temperature_fahrenheit + 
                          bicarbonate + calcium + chloride + creatinine + 
                          glucose + magnesium + potassium + sodium + 
                          hematocrit + wbc, data = icu_train)
y_train <- to_categorical(data.matrix(icu_train$died_within_30d), 2)

#Recombines test data of 30-day mortality TRUE and FALSE
icu_test <- rbind(d30_true[-smpl_true, 1:ncol(icu_analysis)],
                  d30_false[-smpl_false, 1:ncol(icu_analysis)])
#Randomizes the rows
rows <- sample(nrow(icu_test))
icu_test <- icu_test[rows, ]
#creating x_train and y_train for Neural Network/Logistic Regression
x_test <- model.matrix(died_within_30d ~ -1 + gender_F + gender_M +
                          age_at_adm + marital_status_DIVORCED + 
                          marital_status_MARRIED + marital_status_SINGLE + 
                          marital_status_WIDOWED + 
                          ethnicity_AMERICAN.INDIAN.ALASKA.NATIVE + 
                          ethnicity_ASIAN + ethnicity_BLACK.AFRICAN.AMERICAN + 
                          ethnicity_HISPANIC.LATINO + ethnicity_OTHER + 
                          ethnicity_WHITE + ethnicity_UNABLE.TO.OBTAIN + 
                          ethnicity_UNKNOWN + heart_rate + 
                          non_invasive_blood_pressure_systolic + 
                          non_invasive_blood_pressure_mean +
                          respiratory_rate + temperature_fahrenheit + 
                          bicarbonate + calcium + chloride + creatinine + 
                          glucose + magnesium + potassium + sodium + 
                          hematocrit + wbc, data = icu_test)
y_test <- to_categorical(data.matrix(icu_test$died_within_30d), 2)


```
2. Train the models using the training set.
```{r}
#Neural Network

model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 8, activation = 'relu', input_shape = c(30)) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_dense(units = 4, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 2, activation = 'softmax')
summary(model)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
model

system.time({
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
})

plot(history)
```


```{r}
# Logistic Regression
mlogit <- keras_model_sequential() 
mlogit %>% layer_dense(units = 2, activation = 'softmax', input_shape = c(30))
summary(mlogit)

mlogit %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
mlogit

mlogit_history <- mlogit %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)

plot(mlogit_history)
```

3. Compare model prediction performance on the test set.
```{r}
#Neural Network
model %>% evaluate(x_test, y_test)
```

```{r}
#Logistic Regression
mlogit %>% evaluate(x_test, y_test)
```
**Solution:**
The neural network appears to have a smaller loss and slightly greater accuracy than the logistic regression. I would consider the neural network to have better performance in this case. 




